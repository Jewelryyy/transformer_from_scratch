2025-11-06 20:22:57 - INFO - Training Configuration:
2025-11-06 20:22:57 - INFO - d_model: 128
2025-11-06 20:22:57 - INFO - n_head: 4
2025-11-06 20:22:57 - INFO - n_enc_layers: 2
2025-11-06 20:22:57 - INFO - n_dec_layers: 2
2025-11-06 20:22:57 - INFO - d_ff: 512
2025-11-06 20:22:57 - INFO - dropout: 0.1
2025-11-06 20:22:57 - INFO - batch_size: 16
2025-11-06 20:22:57 - INFO - epochs: 5
2025-11-06 20:22:57 - INFO - lr: 0.001
2025-11-06 20:22:57 - INFO - seed: 42
2025-11-06 20:22:58 - WARNING - Using the latest cached version of the module from /home/ic611/.cache/huggingface/modules/datasets_modules/datasets/iwslt2017/03ce9110373117c6f6687719f49f269486a8cd49dcad2527993a316cd4b6ad49 (last modified on Mon Nov  3 20:00:35 2025) since it couldn't be found locally at iwslt2017, or remotely on the Hugging Face Hub.
2025-11-06 20:23:13 - INFO - vocab size: 321997
2025-11-06 20:24:03 - INFO - Epoch 1/5, Batch 500, Loss: 3.6524
2025-11-06 20:24:47 - INFO - Epoch 1/5, Batch 1000, Loss: 3.3857
2025-11-06 20:25:30 - INFO - Epoch 1/5, Batch 1500, Loss: 3.2418
2025-11-06 20:26:13 - INFO - Epoch 1/5, Batch 2000, Loss: 3.1508
2025-11-06 20:26:56 - INFO - Epoch 1/5, Batch 2500, Loss: 3.0806
2025-11-06 20:27:40 - INFO - Epoch 1/5, Batch 3000, Loss: 3.0196
2025-11-06 20:28:23 - INFO - Epoch 1/5, Batch 3500, Loss: 2.9767
2025-11-06 20:29:06 - INFO - Epoch 1/5, Batch 4000, Loss: 2.9351
2025-11-06 20:29:49 - INFO - Epoch 1/5, Batch 4500, Loss: 2.9050
2025-11-06 20:30:32 - INFO - Epoch 1/5, Batch 5000, Loss: 2.8803
2025-11-06 20:31:15 - INFO - Epoch 1/5, Batch 5500, Loss: 2.8538
2025-11-06 20:32:00 - INFO - Epoch 1/5, Batch 6000, Loss: 2.8276
2025-11-06 20:32:43 - INFO - Epoch 1/5, Batch 6500, Loss: 2.8056
2025-11-06 20:33:27 - INFO - Epoch 1/5, Batch 7000, Loss: 2.7836
2025-11-06 20:34:12 - INFO - Epoch 1/5, Batch 7500, Loss: 2.7640
2025-11-06 20:34:57 - INFO - Epoch 1/5, Batch 8000, Loss: 2.7479
2025-11-06 20:35:41 - INFO - Epoch 1/5, Batch 8500, Loss: 2.7315
2025-11-06 20:36:25 - INFO - Epoch 1/5, Batch 9000, Loss: 2.7153
2025-11-06 20:37:08 - INFO - Epoch 1/5, Batch 9500, Loss: 2.6973
2025-11-06 20:37:52 - INFO - Epoch 1/5, Batch 10000, Loss: 2.6821
2025-11-06 20:38:34 - INFO - Epoch 1/5, Batch 10500, Loss: 2.6670
2025-11-06 20:39:17 - INFO - Epoch 1/5, Batch 11000, Loss: 2.6531
2025-11-06 20:40:00 - INFO - Epoch 1/5, Batch 11500, Loss: 2.6395
2025-11-06 20:40:43 - INFO - Epoch 1/5, Batch 12000, Loss: 2.6284
2025-11-06 20:41:27 - INFO - Epoch 1/5, Batch 12500, Loss: 2.6155
2025-11-06 20:42:01 - INFO - Epoch 1/5, Train Loss: 2.6047, Validation Loss: 2.6155, Accuracy: 0.2698, Time: 1123.09s
2025-11-06 20:42:03 - INFO - Saved best model to ./results/20251106_202257/best_transformer.pt
2025-11-06 20:42:03 - INFO - Best Validation Loss: 2.2714, Best PPL: 9.692568404066007
2025-11-06 20:42:47 - INFO - Epoch 2/5, Batch 500, Loss: 2.0901
2025-11-06 20:43:29 - INFO - Epoch 2/5, Batch 1000, Loss: 2.0941
2025-11-06 20:44:13 - INFO - Epoch 2/5, Batch 1500, Loss: 2.0940
2025-11-06 20:44:56 - INFO - Epoch 2/5, Batch 2000, Loss: 2.0945
2025-11-06 20:45:40 - INFO - Epoch 2/5, Batch 2500, Loss: 2.0874
2025-11-06 20:46:25 - INFO - Epoch 2/5, Batch 3000, Loss: 2.0816
2025-11-06 20:47:08 - INFO - Epoch 2/5, Batch 3500, Loss: 2.0851
2025-11-06 20:47:51 - INFO - Epoch 2/5, Batch 4000, Loss: 2.0825
2025-11-06 20:48:35 - INFO - Epoch 2/5, Batch 4500, Loss: 2.0820
2025-11-06 20:49:17 - INFO - Epoch 2/5, Batch 5000, Loss: 2.0810
2025-11-06 20:50:01 - INFO - Epoch 2/5, Batch 5500, Loss: 2.0796
2025-11-06 20:50:44 - INFO - Epoch 2/5, Batch 6000, Loss: 2.0775
2025-11-06 20:51:28 - INFO - Epoch 2/5, Batch 6500, Loss: 2.0769
2025-11-06 20:52:11 - INFO - Epoch 2/5, Batch 7000, Loss: 2.0774
2025-11-06 20:52:55 - INFO - Epoch 2/5, Batch 7500, Loss: 2.0759
2025-11-06 20:53:40 - INFO - Epoch 2/5, Batch 8000, Loss: 2.0734
2025-11-06 20:54:24 - INFO - Epoch 2/5, Batch 8500, Loss: 2.0703
2025-11-06 20:55:08 - INFO - Epoch 2/5, Batch 9000, Loss: 2.0692
2025-11-06 20:55:52 - INFO - Epoch 2/5, Batch 9500, Loss: 2.0686
2025-11-06 20:56:35 - INFO - Epoch 2/5, Batch 10000, Loss: 2.0667
2025-11-06 20:57:18 - INFO - Epoch 2/5, Batch 10500, Loss: 2.0659
2025-11-06 20:58:01 - INFO - Epoch 2/5, Batch 11000, Loss: 2.0662
2025-11-06 20:58:46 - INFO - Epoch 2/5, Batch 11500, Loss: 2.0635
2025-11-06 20:59:29 - INFO - Epoch 2/5, Batch 12000, Loss: 2.0612
2025-11-06 21:00:12 - INFO - Epoch 2/5, Batch 12500, Loss: 2.0586
2025-11-06 21:00:46 - INFO - Epoch 2/5, Train Loss: 2.0582, Validation Loss: 2.0586, Accuracy: 0.3245, Time: 1121.29s
2025-11-06 21:00:48 - INFO - Saved best model to ./results/20251106_202257/best_transformer.pt
2025-11-06 21:00:48 - INFO - Best Validation Loss: 2.0099, Best PPL: 7.462847931782068
2025-11-06 21:01:32 - INFO - Epoch 3/5, Batch 500, Loss: 1.7623
2025-11-06 21:02:14 - INFO - Epoch 3/5, Batch 1000, Loss: 1.7807
2025-11-06 21:02:57 - INFO - Epoch 3/5, Batch 1500, Loss: 1.7903
2025-11-06 21:03:40 - INFO - Epoch 3/5, Batch 2000, Loss: 1.7957
2025-11-06 21:04:23 - INFO - Epoch 3/5, Batch 2500, Loss: 1.7967
2025-11-06 21:05:06 - INFO - Epoch 3/5, Batch 3000, Loss: 1.7996
2025-11-06 21:05:49 - INFO - Epoch 3/5, Batch 3500, Loss: 1.8004
2025-11-06 21:06:32 - INFO - Epoch 3/5, Batch 4000, Loss: 1.8027
2025-11-06 21:07:15 - INFO - Epoch 3/5, Batch 4500, Loss: 1.8055
2025-11-06 21:07:58 - INFO - Epoch 3/5, Batch 5000, Loss: 1.8081
2025-11-06 21:08:41 - INFO - Epoch 3/5, Batch 5500, Loss: 1.8090
2025-11-06 21:09:24 - INFO - Epoch 3/5, Batch 6000, Loss: 1.8117
2025-11-06 21:10:07 - INFO - Epoch 3/5, Batch 6500, Loss: 1.8127
2025-11-06 21:10:51 - INFO - Epoch 3/5, Batch 7000, Loss: 1.8141
2025-11-06 21:11:34 - INFO - Epoch 3/5, Batch 7500, Loss: 1.8138
2025-11-06 21:12:18 - INFO - Epoch 3/5, Batch 8000, Loss: 1.8148
2025-11-06 21:13:01 - INFO - Epoch 3/5, Batch 8500, Loss: 1.8174
2025-11-06 21:13:44 - INFO - Epoch 3/5, Batch 9000, Loss: 1.8187
2025-11-06 21:14:27 - INFO - Epoch 3/5, Batch 9500, Loss: 1.8191
2025-11-06 21:15:10 - INFO - Epoch 3/5, Batch 10000, Loss: 1.8210
2025-11-06 21:15:53 - INFO - Epoch 3/5, Batch 10500, Loss: 1.8226
2025-11-06 21:16:37 - INFO - Epoch 3/5, Batch 11000, Loss: 1.8232
2025-11-06 21:17:20 - INFO - Epoch 3/5, Batch 11500, Loss: 1.8238
2025-11-06 21:18:04 - INFO - Epoch 3/5, Batch 12000, Loss: 1.8232
2025-11-06 21:18:47 - INFO - Epoch 3/5, Batch 12500, Loss: 1.8238
2025-11-06 21:19:21 - INFO - Epoch 3/5, Train Loss: 1.8243, Validation Loss: 1.8238, Accuracy: 0.3543, Time: 1111.20s
2025-11-06 21:19:23 - INFO - Saved best model to ./results/20251106_202257/best_transformer.pt
2025-11-06 21:19:23 - INFO - Best Validation Loss: 1.8651, Best PPL: 6.456652582083331
2025-11-06 21:20:06 - INFO - Epoch 4/5, Batch 500, Loss: 1.5908
2025-11-06 21:20:49 - INFO - Epoch 4/5, Batch 1000, Loss: 1.6004
2025-11-06 21:21:32 - INFO - Epoch 4/5, Batch 1500, Loss: 1.6064
2025-11-06 21:22:15 - INFO - Epoch 4/5, Batch 2000, Loss: 1.6077
2025-11-06 21:22:58 - INFO - Epoch 4/5, Batch 2500, Loss: 1.6123
2025-11-06 21:23:42 - INFO - Epoch 4/5, Batch 3000, Loss: 1.6172
2025-11-06 21:24:24 - INFO - Epoch 4/5, Batch 3500, Loss: 1.6222
2025-11-06 21:25:07 - INFO - Epoch 4/5, Batch 4000, Loss: 1.6301
2025-11-06 21:25:50 - INFO - Epoch 4/5, Batch 4500, Loss: 1.6339
2025-11-06 21:26:33 - INFO - Epoch 4/5, Batch 5000, Loss: 1.6402
2025-11-06 21:27:16 - INFO - Epoch 4/5, Batch 5500, Loss: 1.6437
2025-11-06 21:27:59 - INFO - Epoch 4/5, Batch 6000, Loss: 1.6457
2025-11-06 21:28:42 - INFO - Epoch 4/5, Batch 6500, Loss: 1.6512
2025-11-06 21:29:25 - INFO - Epoch 4/5, Batch 7000, Loss: 1.6557
2025-11-06 21:30:08 - INFO - Epoch 4/5, Batch 7500, Loss: 1.6603
2025-11-06 21:30:51 - INFO - Epoch 4/5, Batch 8000, Loss: 1.6621
2025-11-06 21:31:35 - INFO - Epoch 4/5, Batch 8500, Loss: 1.6641
2025-11-06 21:32:18 - INFO - Epoch 4/5, Batch 9000, Loss: 1.6664
2025-11-06 21:33:01 - INFO - Epoch 4/5, Batch 9500, Loss: 1.6695
2025-11-06 21:33:44 - INFO - Epoch 4/5, Batch 10000, Loss: 1.6725
2025-11-06 21:34:27 - INFO - Epoch 4/5, Batch 10500, Loss: 1.6740
2025-11-06 21:35:10 - INFO - Epoch 4/5, Batch 11000, Loss: 1.6755
2025-11-06 21:35:53 - INFO - Epoch 4/5, Batch 11500, Loss: 1.6775
2025-11-06 21:36:36 - INFO - Epoch 4/5, Batch 12000, Loss: 1.6794
2025-11-06 21:37:18 - INFO - Epoch 4/5, Batch 12500, Loss: 1.6819
2025-11-06 21:37:53 - INFO - Epoch 4/5, Train Loss: 1.6825, Validation Loss: 1.6819, Accuracy: 0.3772, Time: 1108.08s
2025-11-06 21:37:55 - INFO - Saved best model to ./results/20251106_202257/best_transformer.pt
2025-11-06 21:37:55 - INFO - Best Validation Loss: 1.7839, Best PPL: 5.952944824556924
2025-11-06 21:38:39 - INFO - Epoch 5/5, Batch 500, Loss: 1.4332
2025-11-06 21:39:22 - INFO - Epoch 5/5, Batch 1000, Loss: 1.4626
2025-11-06 21:40:05 - INFO - Epoch 5/5, Batch 1500, Loss: 1.4694
2025-11-06 21:40:48 - INFO - Epoch 5/5, Batch 2000, Loss: 1.4771
2025-11-06 21:41:32 - INFO - Epoch 5/5, Batch 2500, Loss: 1.4797
2025-11-06 21:42:15 - INFO - Epoch 5/5, Batch 3000, Loss: 1.4876
2025-11-06 21:42:58 - INFO - Epoch 5/5, Batch 3500, Loss: 1.4959
2025-11-06 21:43:41 - INFO - Epoch 5/5, Batch 4000, Loss: 1.5023
2025-11-06 21:44:24 - INFO - Epoch 5/5, Batch 4500, Loss: 1.5088
2025-11-06 21:45:07 - INFO - Epoch 5/5, Batch 5000, Loss: 1.5148
2025-11-06 21:45:51 - INFO - Epoch 5/5, Batch 5500, Loss: 1.5197
2025-11-06 21:46:34 - INFO - Epoch 5/5, Batch 6000, Loss: 1.5247
2025-11-06 21:47:17 - INFO - Epoch 5/5, Batch 6500, Loss: 1.5309
2025-11-06 21:48:00 - INFO - Epoch 5/5, Batch 7000, Loss: 1.5349
2025-11-06 21:48:43 - INFO - Epoch 5/5, Batch 7500, Loss: 1.5408
2025-11-06 21:49:26 - INFO - Epoch 5/5, Batch 8000, Loss: 1.5432
2025-11-06 21:50:09 - INFO - Epoch 5/5, Batch 8500, Loss: 1.5469
2025-11-06 21:50:52 - INFO - Epoch 5/5, Batch 9000, Loss: 1.5504
2025-11-06 21:51:36 - INFO - Epoch 5/5, Batch 9500, Loss: 1.5522
2025-11-06 21:52:19 - INFO - Epoch 5/5, Batch 10000, Loss: 1.5559
2025-11-06 21:53:02 - INFO - Epoch 5/5, Batch 10500, Loss: 1.5590
2025-11-06 21:53:45 - INFO - Epoch 5/5, Batch 11000, Loss: 1.5615
2025-11-06 21:54:29 - INFO - Epoch 5/5, Batch 11500, Loss: 1.5628
2025-11-06 21:55:12 - INFO - Epoch 5/5, Batch 12000, Loss: 1.5660
2025-11-06 21:55:55 - INFO - Epoch 5/5, Batch 12500, Loss: 1.5687
2025-11-06 21:56:30 - INFO - Epoch 5/5, Train Loss: 1.5710, Validation Loss: 1.5687, Accuracy: 0.3886, Time: 1113.06s
2025-11-06 21:56:32 - INFO - Saved best model to ./results/20251106_202257/best_transformer.pt
2025-11-06 21:56:32 - INFO - Best Validation Loss: 1.7350, Best PPL: 5.6687349656816535
2025-11-06 21:56:33 - INFO - Saved training parameters to ./results/20251106_202257/config.json
