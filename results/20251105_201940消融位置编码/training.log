2025-11-05 20:19:40 - INFO - Training Configuration:
2025-11-05 20:19:40 - INFO - d_model: 128
2025-11-05 20:19:40 - INFO - n_head: 4
2025-11-05 20:19:40 - INFO - n_enc_layers: 2
2025-11-05 20:19:40 - INFO - n_dec_layers: 2
2025-11-05 20:19:40 - INFO - d_ff: 512
2025-11-05 20:19:40 - INFO - dropout: 0.1
2025-11-05 20:19:40 - INFO - batch_size: 16
2025-11-05 20:19:40 - INFO - epochs: 5
2025-11-05 20:19:40 - INFO - lr: 0.001
2025-11-05 20:19:40 - INFO - seed: 42
2025-11-05 20:19:45 - WARNING - Using the latest cached version of the module from /home/ic611/.cache/huggingface/modules/datasets_modules/datasets/iwslt2017/03ce9110373117c6f6687719f49f269486a8cd49dcad2527993a316cd4b6ad49 (last modified on Mon Nov  3 20:00:35 2025) since it couldn't be found locally at iwslt2017, or remotely on the Hugging Face Hub.
2025-11-05 20:20:02 - INFO - vocab size: 10000
2025-11-05 20:20:28 - INFO - Epoch 1/5, Batch 500, Loss: 2.3561
2025-11-05 20:20:50 - INFO - Epoch 1/5, Batch 1000, Loss: 2.1957
2025-11-05 20:21:12 - INFO - Epoch 1/5, Batch 1500, Loss: 2.1098
2025-11-05 20:21:34 - INFO - Epoch 1/5, Batch 2000, Loss: 2.0586
2025-11-05 20:21:58 - INFO - Epoch 1/5, Batch 2500, Loss: 2.0137
2025-11-05 20:22:21 - INFO - Epoch 1/5, Batch 3000, Loss: 1.9781
2025-11-05 20:22:45 - INFO - Epoch 1/5, Batch 3500, Loss: 1.9533
2025-11-05 20:23:08 - INFO - Epoch 1/5, Batch 4000, Loss: 1.9263
2025-11-05 20:23:30 - INFO - Epoch 1/5, Batch 4500, Loss: 1.9038
2025-11-05 20:23:52 - INFO - Epoch 1/5, Batch 5000, Loss: 1.8803
2025-11-05 20:24:15 - INFO - Epoch 1/5, Batch 5500, Loss: 1.8636
2025-11-05 20:24:37 - INFO - Epoch 1/5, Batch 6000, Loss: 1.8474
2025-11-05 20:25:01 - INFO - Epoch 1/5, Batch 6500, Loss: 1.8344
2025-11-05 20:25:25 - INFO - Epoch 1/5, Batch 7000, Loss: 1.8215
2025-11-05 20:25:50 - INFO - Epoch 1/5, Batch 7500, Loss: 1.8068
2025-11-05 20:26:12 - INFO - Epoch 1/5, Batch 8000, Loss: 1.7944
2025-11-05 20:26:34 - INFO - Epoch 1/5, Batch 8500, Loss: 1.7841
2025-11-05 20:26:56 - INFO - Epoch 1/5, Batch 9000, Loss: 1.7732
2025-11-05 20:27:19 - INFO - Epoch 1/5, Batch 9500, Loss: 1.7647
2025-11-05 20:27:42 - INFO - Epoch 1/5, Batch 10000, Loss: 1.7556
2025-11-05 20:28:05 - INFO - Epoch 1/5, Batch 10500, Loss: 1.7474
2025-11-05 20:28:29 - INFO - Epoch 1/5, Batch 11000, Loss: 1.7390
2025-11-05 20:28:52 - INFO - Epoch 1/5, Batch 11500, Loss: 1.7318
2025-11-05 20:29:16 - INFO - Epoch 1/5, Batch 12000, Loss: 1.7238
2025-11-05 20:29:40 - INFO - Epoch 1/5, Batch 12500, Loss: 1.7175
2025-11-05 20:29:58 - INFO - Epoch 1/5, Train Loss: 1.7120, Validation Loss: 1.7175, Accuracy: 0.3096, Time: 593.94s
2025-11-05 20:29:59 - INFO - Saved best model to ./results/20251105_201940/best_transformer.pt
2025-11-05 20:29:59 - INFO - Best Validation Loss: 1.5470, Best PPL: 4.697221321282196
2025-11-05 20:30:22 - INFO - Epoch 2/5, Batch 500, Loss: 1.5136
2025-11-05 20:30:46 - INFO - Epoch 2/5, Batch 1000, Loss: 1.5049
2025-11-05 20:31:09 - INFO - Epoch 2/5, Batch 1500, Loss: 1.5084
2025-11-05 20:31:33 - INFO - Epoch 2/5, Batch 2000, Loss: 1.5064
2025-11-05 20:31:55 - INFO - Epoch 2/5, Batch 2500, Loss: 1.5010
2025-11-05 20:32:17 - INFO - Epoch 2/5, Batch 3000, Loss: 1.4989
2025-11-05 20:32:39 - INFO - Epoch 2/5, Batch 3500, Loss: 1.4987
2025-11-05 20:33:02 - INFO - Epoch 2/5, Batch 4000, Loss: 1.4987
2025-11-05 20:33:27 - INFO - Epoch 2/5, Batch 4500, Loss: 1.4968
2025-11-05 20:33:52 - INFO - Epoch 2/5, Batch 5000, Loss: 1.4944
2025-11-05 20:34:17 - INFO - Epoch 2/5, Batch 5500, Loss: 1.4923
2025-11-05 20:34:42 - INFO - Epoch 2/5, Batch 6000, Loss: 1.4903
2025-11-05 20:35:04 - INFO - Epoch 2/5, Batch 6500, Loss: 1.4881
2025-11-05 20:35:27 - INFO - Epoch 2/5, Batch 7000, Loss: 1.4848
2025-11-05 20:35:52 - INFO - Epoch 2/5, Batch 7500, Loss: 1.4828
2025-11-05 20:36:15 - INFO - Epoch 2/5, Batch 8000, Loss: 1.4810
2025-11-05 20:36:36 - INFO - Epoch 2/5, Batch 8500, Loss: 1.4785
2025-11-05 20:36:58 - INFO - Epoch 2/5, Batch 9000, Loss: 1.4759
2025-11-05 20:37:20 - INFO - Epoch 2/5, Batch 9500, Loss: 1.4751
2025-11-05 20:37:42 - INFO - Epoch 2/5, Batch 10000, Loss: 1.4736
2025-11-05 20:38:03 - INFO - Epoch 2/5, Batch 10500, Loss: 1.4728
2025-11-05 20:38:25 - INFO - Epoch 2/5, Batch 11000, Loss: 1.4704
2025-11-05 20:38:47 - INFO - Epoch 2/5, Batch 11500, Loss: 1.4694
2025-11-05 20:39:08 - INFO - Epoch 2/5, Batch 12000, Loss: 1.4674
2025-11-05 20:39:30 - INFO - Epoch 2/5, Batch 12500, Loss: 1.4658
2025-11-05 20:39:47 - INFO - Epoch 2/5, Train Loss: 1.4644, Validation Loss: 1.4658, Accuracy: 0.3256, Time: 588.28s
2025-11-05 20:39:48 - INFO - Saved best model to ./results/20251105_201940/best_transformer.pt
2025-11-05 20:39:48 - INFO - Best Validation Loss: 1.4332, Best PPL: 4.192279359879339
2025-11-05 20:40:09 - INFO - Epoch 3/5, Batch 500, Loss: 1.3947
2025-11-05 20:40:31 - INFO - Epoch 3/5, Batch 1000, Loss: 1.3987
2025-11-05 20:40:54 - INFO - Epoch 3/5, Batch 1500, Loss: 1.3955
2025-11-05 20:41:18 - INFO - Epoch 3/5, Batch 2000, Loss: 1.3947
2025-11-05 20:41:41 - INFO - Epoch 3/5, Batch 2500, Loss: 1.3954
2025-11-05 20:42:03 - INFO - Epoch 3/5, Batch 3000, Loss: 1.3959
2025-11-05 20:42:27 - INFO - Epoch 3/5, Batch 3500, Loss: 1.3969
2025-11-05 20:42:50 - INFO - Epoch 3/5, Batch 4000, Loss: 1.3993
2025-11-05 20:43:14 - INFO - Epoch 3/5, Batch 4500, Loss: 1.3974
2025-11-05 20:43:37 - INFO - Epoch 3/5, Batch 5000, Loss: 1.3966
2025-11-05 20:44:00 - INFO - Epoch 3/5, Batch 5500, Loss: 1.3964
2025-11-05 20:44:23 - INFO - Epoch 3/5, Batch 6000, Loss: 1.3957
2025-11-05 20:44:45 - INFO - Epoch 3/5, Batch 6500, Loss: 1.3949
2025-11-05 20:45:07 - INFO - Epoch 3/5, Batch 7000, Loss: 1.3943
2025-11-05 20:45:28 - INFO - Epoch 3/5, Batch 7500, Loss: 1.3923
2025-11-05 20:45:54 - INFO - Epoch 3/5, Batch 8000, Loss: 1.3915
2025-11-05 20:46:19 - INFO - Epoch 3/5, Batch 8500, Loss: 1.3906
2025-11-05 20:46:40 - INFO - Epoch 3/5, Batch 9000, Loss: 1.3902
2025-11-05 20:47:02 - INFO - Epoch 3/5, Batch 9500, Loss: 1.3887
2025-11-05 20:47:24 - INFO - Epoch 3/5, Batch 10000, Loss: 1.3885
2025-11-05 20:47:46 - INFO - Epoch 3/5, Batch 10500, Loss: 1.3872
2025-11-05 20:48:07 - INFO - Epoch 3/5, Batch 11000, Loss: 1.3863
2025-11-05 20:48:29 - INFO - Epoch 3/5, Batch 11500, Loss: 1.3851
2025-11-05 20:48:51 - INFO - Epoch 3/5, Batch 12000, Loss: 1.3835
2025-11-05 20:49:13 - INFO - Epoch 3/5, Batch 12500, Loss: 1.3835
2025-11-05 20:49:30 - INFO - Epoch 3/5, Train Loss: 1.3826, Validation Loss: 1.3835, Accuracy: 0.3314, Time: 582.24s
2025-11-05 20:49:30 - INFO - Saved best model to ./results/20251105_201940/best_transformer.pt
2025-11-05 20:49:30 - INFO - Best Validation Loss: 1.3841, Best PPL: 3.991128899366568
2025-11-05 20:49:55 - INFO - Epoch 4/5, Batch 500, Loss: 1.3458
2025-11-05 20:50:20 - INFO - Epoch 4/5, Batch 1000, Loss: 1.3365
2025-11-05 20:50:45 - INFO - Epoch 4/5, Batch 1500, Loss: 1.3328
2025-11-05 20:51:10 - INFO - Epoch 4/5, Batch 2000, Loss: 1.3356
2025-11-05 20:51:35 - INFO - Epoch 4/5, Batch 2500, Loss: 1.3344
2025-11-05 20:51:57 - INFO - Epoch 4/5, Batch 3000, Loss: 1.3327
2025-11-05 20:52:19 - INFO - Epoch 4/5, Batch 3500, Loss: 1.3329
2025-11-05 20:52:41 - INFO - Epoch 4/5, Batch 4000, Loss: 1.3305
2025-11-05 20:53:03 - INFO - Epoch 4/5, Batch 4500, Loss: 1.3309
2025-11-05 20:53:25 - INFO - Epoch 4/5, Batch 5000, Loss: 1.3308
2025-11-05 20:53:47 - INFO - Epoch 4/5, Batch 5500, Loss: 1.3308
2025-11-05 20:54:08 - INFO - Epoch 4/5, Batch 6000, Loss: 1.3300
2025-11-05 20:54:30 - INFO - Epoch 4/5, Batch 6500, Loss: 1.3309
2025-11-05 20:54:52 - INFO - Epoch 4/5, Batch 7000, Loss: 1.3309
2025-11-05 20:55:14 - INFO - Epoch 4/5, Batch 7500, Loss: 1.3301
2025-11-05 20:55:35 - INFO - Epoch 4/5, Batch 8000, Loss: 1.3309
2025-11-05 20:55:57 - INFO - Epoch 4/5, Batch 8500, Loss: 1.3309
2025-11-05 20:56:19 - INFO - Epoch 4/5, Batch 9000, Loss: 1.3306
2025-11-05 20:56:41 - INFO - Epoch 4/5, Batch 9500, Loss: 1.3298
2025-11-05 20:57:03 - INFO - Epoch 4/5, Batch 10000, Loss: 1.3301
2025-11-05 20:57:26 - INFO - Epoch 4/5, Batch 10500, Loss: 1.3292
2025-11-05 20:57:48 - INFO - Epoch 4/5, Batch 11000, Loss: 1.3293
2025-11-05 20:58:10 - INFO - Epoch 4/5, Batch 11500, Loss: 1.3302
2025-11-05 20:58:32 - INFO - Epoch 4/5, Batch 12000, Loss: 1.3301
2025-11-05 20:58:54 - INFO - Epoch 4/5, Batch 12500, Loss: 1.3296
2025-11-05 20:59:11 - INFO - Epoch 4/5, Train Loss: 1.3290, Validation Loss: 1.3296, Accuracy: 0.3421, Time: 580.31s
2025-11-05 20:59:12 - INFO - Saved best model to ./results/20251105_201940/best_transformer.pt
2025-11-05 20:59:12 - INFO - Best Validation Loss: 1.3432, Best PPL: 3.831329002562799
2025-11-05 20:59:34 - INFO - Epoch 5/5, Batch 500, Loss: 1.2819
2025-11-05 20:59:56 - INFO - Epoch 5/5, Batch 1000, Loss: 1.2771
2025-11-05 21:00:18 - INFO - Epoch 5/5, Batch 1500, Loss: 1.2793
2025-11-05 21:00:40 - INFO - Epoch 5/5, Batch 2000, Loss: 1.2795
2025-11-05 21:01:02 - INFO - Epoch 5/5, Batch 2500, Loss: 1.2858
2025-11-05 21:01:24 - INFO - Epoch 5/5, Batch 3000, Loss: 1.2876
2025-11-05 21:01:46 - INFO - Epoch 5/5, Batch 3500, Loss: 1.2894
2025-11-05 21:02:10 - INFO - Epoch 5/5, Batch 4000, Loss: 1.2913
2025-11-05 21:02:33 - INFO - Epoch 5/5, Batch 4500, Loss: 1.2934
2025-11-05 21:02:57 - INFO - Epoch 5/5, Batch 5000, Loss: 1.2914
2025-11-05 21:03:20 - INFO - Epoch 5/5, Batch 5500, Loss: 1.2927
2025-11-05 21:03:42 - INFO - Epoch 5/5, Batch 6000, Loss: 1.2940
2025-11-05 21:04:04 - INFO - Epoch 5/5, Batch 6500, Loss: 1.2916
2025-11-05 21:04:25 - INFO - Epoch 5/5, Batch 7000, Loss: 1.2915
2025-11-05 21:04:47 - INFO - Epoch 5/5, Batch 7500, Loss: 1.2918
2025-11-05 21:05:09 - INFO - Epoch 5/5, Batch 8000, Loss: 1.2925
2025-11-05 21:05:30 - INFO - Epoch 5/5, Batch 8500, Loss: 1.2932
2025-11-05 21:05:52 - INFO - Epoch 5/5, Batch 9000, Loss: 1.2936
2025-11-05 21:06:15 - INFO - Epoch 5/5, Batch 9500, Loss: 1.2937
2025-11-05 21:06:39 - INFO - Epoch 5/5, Batch 10000, Loss: 1.2935
2025-11-05 21:07:00 - INFO - Epoch 5/5, Batch 10500, Loss: 1.2935
2025-11-05 21:07:24 - INFO - Epoch 5/5, Batch 11000, Loss: 1.2944
2025-11-05 21:07:47 - INFO - Epoch 5/5, Batch 11500, Loss: 1.2950
2025-11-05 21:08:11 - INFO - Epoch 5/5, Batch 12000, Loss: 1.2955
2025-11-05 21:08:34 - INFO - Epoch 5/5, Batch 12500, Loss: 1.2949
2025-11-05 21:08:53 - INFO - Epoch 5/5, Train Loss: 1.2947, Validation Loss: 1.2949, Accuracy: 0.3409, Time: 580.37s
2025-11-05 21:08:53 - INFO - Saved best model to ./results/20251105_201940/best_transformer.pt
2025-11-05 21:08:53 - INFO - Best Validation Loss: 1.3250, Best PPL: 3.762011234385532
2025-11-05 21:08:53 - INFO - Saved training parameters to ./results/20251105_201940/config.json
