2025-11-07 15:02:00 - INFO - Training Configuration:
2025-11-07 15:02:00 - INFO - d_model: 128
2025-11-07 15:02:00 - INFO - n_head: 4
2025-11-07 15:02:00 - INFO - n_enc_layers: 2
2025-11-07 15:02:00 - INFO - n_dec_layers: 2
2025-11-07 15:02:00 - INFO - d_ff: 512
2025-11-07 15:02:00 - INFO - dropout: 0.1
2025-11-07 15:02:00 - INFO - batch_size: 16
2025-11-07 15:02:00 - INFO - epochs: 5
2025-11-07 15:02:00 - INFO - lr: 0.001
2025-11-07 15:02:00 - INFO - seed: 42
2025-11-07 15:02:00 - WARNING - Using the latest cached version of the module from /home/ic611/.cache/huggingface/modules/datasets_modules/datasets/iwslt2017/03ce9110373117c6f6687719f49f269486a8cd49dcad2527993a316cd4b6ad49 (last modified on Mon Nov  3 20:00:35 2025) since it couldn't be found locally at iwslt2017, or remotely on the Hugging Face Hub.
2025-11-07 15:02:15 - INFO - vocab size: 321997
2025-11-07 15:03:03 - INFO - Epoch 1/5, Batch 500, Loss: 3.6395
2025-11-07 15:03:46 - INFO - Epoch 1/5, Batch 1000, Loss: 3.3920
2025-11-07 15:04:29 - INFO - Epoch 1/5, Batch 1500, Loss: 3.2623
2025-11-07 15:05:12 - INFO - Epoch 1/5, Batch 2000, Loss: 3.1839
2025-11-07 15:05:55 - INFO - Epoch 1/5, Batch 2500, Loss: 3.1237
2025-11-07 15:06:39 - INFO - Epoch 1/5, Batch 3000, Loss: 3.0709
2025-11-07 15:07:22 - INFO - Epoch 1/5, Batch 3500, Loss: 3.0356
2025-11-07 15:08:05 - INFO - Epoch 1/5, Batch 4000, Loss: 3.0004
2025-11-07 15:08:48 - INFO - Epoch 1/5, Batch 4500, Loss: 2.9763
2025-11-07 15:09:31 - INFO - Epoch 1/5, Batch 5000, Loss: 2.9571
2025-11-07 15:10:14 - INFO - Epoch 1/5, Batch 5500, Loss: 2.9355
2025-11-07 15:10:57 - INFO - Epoch 1/5, Batch 6000, Loss: 2.9137
2025-11-07 15:11:40 - INFO - Epoch 1/5, Batch 6500, Loss: 2.8962
2025-11-07 15:12:23 - INFO - Epoch 1/5, Batch 7000, Loss: 2.8782
2025-11-07 15:13:07 - INFO - Epoch 1/5, Batch 7500, Loss: 2.8624
2025-11-07 15:13:50 - INFO - Epoch 1/5, Batch 8000, Loss: 2.8503
2025-11-07 15:14:33 - INFO - Epoch 1/5, Batch 8500, Loss: 2.8376
2025-11-07 15:15:16 - INFO - Epoch 1/5, Batch 9000, Loss: 2.8250
2025-11-07 15:16:00 - INFO - Epoch 1/5, Batch 9500, Loss: 2.8104
2025-11-07 15:16:43 - INFO - Epoch 1/5, Batch 10000, Loss: 2.7987
2025-11-07 15:17:27 - INFO - Epoch 1/5, Batch 10500, Loss: 2.7869
2025-11-07 15:18:12 - INFO - Epoch 1/5, Batch 11000, Loss: 2.7762
2025-11-07 15:18:55 - INFO - Epoch 1/5, Batch 11500, Loss: 2.7657
2025-11-07 15:19:38 - INFO - Epoch 1/5, Batch 12000, Loss: 2.7576
2025-11-07 15:20:22 - INFO - Epoch 1/5, Batch 12500, Loss: 2.7475
2025-11-07 15:20:57 - INFO - Epoch 1/5, Train Loss: 2.7388, Validation Loss: 2.5266, Accuracy: 0.1938, Time: 1116.70s
2025-11-07 15:20:59 - INFO - Saved best model to ./results/20251107_150200/best_transformer.pt
2025-11-07 15:20:59 - INFO - Best Validation Loss: 2.5266, Best PPL: 12.510501380352189
2025-11-07 15:21:42 - INFO - Epoch 2/5, Batch 500, Loss: 2.3047
2025-11-07 15:22:25 - INFO - Epoch 2/5, Batch 1000, Loss: 2.3093
2025-11-07 15:23:09 - INFO - Epoch 2/5, Batch 1500, Loss: 2.3109
2025-11-07 15:23:52 - INFO - Epoch 2/5, Batch 2000, Loss: 2.3131
2025-11-07 15:24:37 - INFO - Epoch 2/5, Batch 2500, Loss: 2.3066
2025-11-07 15:25:22 - INFO - Epoch 2/5, Batch 3000, Loss: 2.3013
2025-11-07 15:26:06 - INFO - Epoch 2/5, Batch 3500, Loss: 2.3062
2025-11-07 15:26:51 - INFO - Epoch 2/5, Batch 4000, Loss: 2.3047
2025-11-07 15:27:36 - INFO - Epoch 2/5, Batch 4500, Loss: 2.3056
2025-11-07 15:28:21 - INFO - Epoch 2/5, Batch 5000, Loss: 2.3062
2025-11-07 15:29:04 - INFO - Epoch 2/5, Batch 5500, Loss: 2.3063
2025-11-07 15:29:48 - INFO - Epoch 2/5, Batch 6000, Loss: 2.3055
2025-11-07 15:30:31 - INFO - Epoch 2/5, Batch 6500, Loss: 2.3063
2025-11-07 15:31:15 - INFO - Epoch 2/5, Batch 7000, Loss: 2.3082
2025-11-07 15:32:00 - INFO - Epoch 2/5, Batch 7500, Loss: 2.3081
2025-11-07 15:32:44 - INFO - Epoch 2/5, Batch 8000, Loss: 2.3071
2025-11-07 15:33:27 - INFO - Epoch 2/5, Batch 8500, Loss: 2.3055
2025-11-07 15:34:12 - INFO - Epoch 2/5, Batch 9000, Loss: 2.3058
2025-11-07 15:34:56 - INFO - Epoch 2/5, Batch 9500, Loss: 2.3070
2025-11-07 15:35:41 - INFO - Epoch 2/5, Batch 10000, Loss: 2.3064
2025-11-07 15:36:25 - INFO - Epoch 2/5, Batch 10500, Loss: 2.3072
2025-11-07 15:37:09 - INFO - Epoch 2/5, Batch 11000, Loss: 2.3091
2025-11-07 15:37:52 - INFO - Epoch 2/5, Batch 11500, Loss: 2.3077
2025-11-07 15:38:35 - INFO - Epoch 2/5, Batch 12000, Loss: 2.3069
2025-11-07 15:39:18 - INFO - Epoch 2/5, Batch 12500, Loss: 2.3055
2025-11-07 15:39:53 - INFO - Epoch 2/5, Train Loss: 2.3064, Validation Loss: 2.3699, Accuracy: 0.2207, Time: 1132.12s
2025-11-07 15:39:55 - INFO - Saved best model to ./results/20251107_150200/best_transformer.pt
2025-11-07 15:39:55 - INFO - Best Validation Loss: 2.3699, Best PPL: 10.696830663392575
2025-11-07 15:40:38 - INFO - Epoch 3/5, Batch 500, Loss: 2.0525
2025-11-07 15:41:20 - INFO - Epoch 3/5, Batch 1000, Loss: 2.0751
2025-11-07 15:42:03 - INFO - Epoch 3/5, Batch 1500, Loss: 2.0860
2025-11-07 15:42:46 - INFO - Epoch 3/5, Batch 2000, Loss: 2.0911
2025-11-07 15:43:29 - INFO - Epoch 3/5, Batch 2500, Loss: 2.0914
2025-11-07 15:44:12 - INFO - Epoch 3/5, Batch 3000, Loss: 2.0948
2025-11-07 15:44:55 - INFO - Epoch 3/5, Batch 3500, Loss: 2.0967
2025-11-07 15:45:39 - INFO - Epoch 3/5, Batch 4000, Loss: 2.0991
2025-11-07 15:46:21 - INFO - Epoch 3/5, Batch 4500, Loss: 2.1025
2025-11-07 15:47:04 - INFO - Epoch 3/5, Batch 5000, Loss: 2.1057
2025-11-07 15:47:48 - INFO - Epoch 3/5, Batch 5500, Loss: 2.1075
2025-11-07 15:48:31 - INFO - Epoch 3/5, Batch 6000, Loss: 2.1116
2025-11-07 15:49:14 - INFO - Epoch 3/5, Batch 6500, Loss: 2.1134
2025-11-07 15:49:58 - INFO - Epoch 3/5, Batch 7000, Loss: 2.1154
2025-11-07 15:50:41 - INFO - Epoch 3/5, Batch 7500, Loss: 2.1155
2025-11-07 15:51:25 - INFO - Epoch 3/5, Batch 8000, Loss: 2.1174
2025-11-07 15:52:08 - INFO - Epoch 3/5, Batch 8500, Loss: 2.1208
2025-11-07 15:52:52 - INFO - Epoch 3/5, Batch 9000, Loss: 2.1228
2025-11-07 15:53:35 - INFO - Epoch 3/5, Batch 9500, Loss: 2.1238
2025-11-07 15:54:18 - INFO - Epoch 3/5, Batch 10000, Loss: 2.1267
2025-11-07 15:55:01 - INFO - Epoch 3/5, Batch 10500, Loss: 2.1291
2025-11-07 15:55:44 - INFO - Epoch 3/5, Batch 11000, Loss: 2.1305
2025-11-07 15:56:28 - INFO - Epoch 3/5, Batch 11500, Loss: 2.1320
2025-11-07 15:57:11 - INFO - Epoch 3/5, Batch 12000, Loss: 2.1320
2025-11-07 15:57:54 - INFO - Epoch 3/5, Batch 12500, Loss: 2.1336
2025-11-07 15:58:29 - INFO - Epoch 3/5, Train Loss: 2.1349, Validation Loss: 2.2803, Accuracy: 0.2360, Time: 1112.53s
2025-11-07 15:58:31 - INFO - Saved best model to ./results/20251107_150200/best_transformer.pt
2025-11-07 15:58:31 - INFO - Best Validation Loss: 2.2803, Best PPL: 9.77985692204842
2025-11-07 15:59:14 - INFO - Epoch 4/5, Batch 500, Loss: 1.9328
2025-11-07 15:59:57 - INFO - Epoch 4/5, Batch 1000, Loss: 1.9451
2025-11-07 16:00:40 - INFO - Epoch 4/5, Batch 1500, Loss: 1.9484
2025-11-07 16:01:23 - INFO - Epoch 4/5, Batch 2000, Loss: 1.9476
2025-11-07 16:02:07 - INFO - Epoch 4/5, Batch 2500, Loss: 1.9526
2025-11-07 16:02:50 - INFO - Epoch 4/5, Batch 3000, Loss: 1.9581
2025-11-07 16:03:33 - INFO - Epoch 4/5, Batch 3500, Loss: 1.9630
2025-11-07 16:04:16 - INFO - Epoch 4/5, Batch 4000, Loss: 1.9713
2025-11-07 16:04:59 - INFO - Epoch 4/5, Batch 4500, Loss: 1.9746
2025-11-07 16:05:43 - INFO - Epoch 4/5, Batch 5000, Loss: 1.9816
2025-11-07 16:06:26 - INFO - Epoch 4/5, Batch 5500, Loss: 1.9850
2025-11-07 16:07:09 - INFO - Epoch 4/5, Batch 6000, Loss: 1.9869
2025-11-07 16:07:52 - INFO - Epoch 4/5, Batch 6500, Loss: 1.9926
2025-11-07 16:08:36 - INFO - Epoch 4/5, Batch 7000, Loss: 1.9975
2025-11-07 16:09:19 - INFO - Epoch 4/5, Batch 7500, Loss: 2.0023
2025-11-07 16:10:02 - INFO - Epoch 4/5, Batch 8000, Loss: 2.0039
2025-11-07 16:10:46 - INFO - Epoch 4/5, Batch 8500, Loss: 2.0062
2025-11-07 16:11:29 - INFO - Epoch 4/5, Batch 9000, Loss: 2.0087
2025-11-07 16:12:12 - INFO - Epoch 4/5, Batch 9500, Loss: 2.0119
2025-11-07 16:12:55 - INFO - Epoch 4/5, Batch 10000, Loss: 2.0154
2025-11-07 16:13:38 - INFO - Epoch 4/5, Batch 10500, Loss: 2.0170
2025-11-07 16:14:21 - INFO - Epoch 4/5, Batch 11000, Loss: 2.0189
2025-11-07 16:15:04 - INFO - Epoch 4/5, Batch 11500, Loss: 2.0214
2025-11-07 16:15:47 - INFO - Epoch 4/5, Batch 12000, Loss: 2.0238
2025-11-07 16:16:30 - INFO - Epoch 4/5, Batch 12500, Loss: 2.0267
2025-11-07 16:17:05 - INFO - Epoch 4/5, Train Loss: 2.0274, Validation Loss: 2.2226, Accuracy: 0.2461, Time: 1112.06s
2025-11-07 16:17:07 - INFO - Saved best model to ./results/20251107_150200/best_transformer.pt
2025-11-07 16:17:07 - INFO - Best Validation Loss: 2.2226, Best PPL: 9.231729888975911
2025-11-07 16:17:50 - INFO - Epoch 5/5, Batch 500, Loss: 1.7879
2025-11-07 16:18:33 - INFO - Epoch 5/5, Batch 1000, Loss: 1.8188
2025-11-07 16:19:17 - INFO - Epoch 5/5, Batch 1500, Loss: 1.8275
2025-11-07 16:20:00 - INFO - Epoch 5/5, Batch 2000, Loss: 1.8352
2025-11-07 16:20:44 - INFO - Epoch 5/5, Batch 2500, Loss: 1.8375
2025-11-07 16:21:27 - INFO - Epoch 5/5, Batch 3000, Loss: 1.8447
2025-11-07 16:22:10 - INFO - Epoch 5/5, Batch 3500, Loss: 1.8534
2025-11-07 16:22:53 - INFO - Epoch 5/5, Batch 4000, Loss: 1.8592
2025-11-07 16:23:37 - INFO - Epoch 5/5, Batch 4500, Loss: 1.8658
2025-11-07 16:24:20 - INFO - Epoch 5/5, Batch 5000, Loss: 1.8724
2025-11-07 16:25:03 - INFO - Epoch 5/5, Batch 5500, Loss: 1.8774
2025-11-07 16:25:46 - INFO - Epoch 5/5, Batch 6000, Loss: 1.8821
2025-11-07 16:26:29 - INFO - Epoch 5/5, Batch 6500, Loss: 1.8883
2025-11-07 16:27:12 - INFO - Epoch 5/5, Batch 7000, Loss: 1.8926
2025-11-07 16:27:55 - INFO - Epoch 5/5, Batch 7500, Loss: 1.8986
2025-11-07 16:28:38 - INFO - Epoch 5/5, Batch 8000, Loss: 1.9008
2025-11-07 16:29:21 - INFO - Epoch 5/5, Batch 8500, Loss: 1.9048
2025-11-07 16:30:05 - INFO - Epoch 5/5, Batch 9000, Loss: 1.9083
2025-11-07 16:30:48 - INFO - Epoch 5/5, Batch 9500, Loss: 1.9097
2025-11-07 16:31:33 - INFO - Epoch 5/5, Batch 10000, Loss: 1.9135
2025-11-07 16:32:16 - INFO - Epoch 5/5, Batch 10500, Loss: 1.9166
2025-11-07 16:33:01 - INFO - Epoch 5/5, Batch 11000, Loss: 1.9192
2025-11-07 16:33:47 - INFO - Epoch 5/5, Batch 11500, Loss: 1.9203
2025-11-07 16:34:30 - INFO - Epoch 5/5, Batch 12000, Loss: 1.9239
2025-11-07 16:35:13 - INFO - Epoch 5/5, Batch 12500, Loss: 1.9268
2025-11-07 16:35:48 - INFO - Epoch 5/5, Train Loss: 1.9293, Validation Loss: 2.1921, Accuracy: 0.2524, Time: 1119.36s
2025-11-07 16:35:50 - INFO - Saved best model to ./results/20251107_150200/best_transformer.pt
2025-11-07 16:35:50 - INFO - Best Validation Loss: 2.1921, Best PPL: 8.954342668059564
2025-11-07 16:35:50 - INFO - Saved training parameters to ./results/20251107_150200/config.json
