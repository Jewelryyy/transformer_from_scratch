2025-11-05 22:11:06 - INFO - Training Configuration:
2025-11-05 22:11:06 - INFO - d_model: 128
2025-11-05 22:11:06 - INFO - n_head: 2
2025-11-05 22:11:06 - INFO - n_enc_layers: 2
2025-11-05 22:11:06 - INFO - n_dec_layers: 2
2025-11-05 22:11:06 - INFO - d_ff: 512
2025-11-05 22:11:06 - INFO - dropout: 0.1
2025-11-05 22:11:06 - INFO - batch_size: 16
2025-11-05 22:11:06 - INFO - epochs: 5
2025-11-05 22:11:06 - INFO - lr: 0.001
2025-11-05 22:11:06 - INFO - seed: 42
2025-11-05 22:11:11 - WARNING - Using the latest cached version of the module from /home/ic611/.cache/huggingface/modules/datasets_modules/datasets/iwslt2017/03ce9110373117c6f6687719f49f269486a8cd49dcad2527993a316cd4b6ad49 (last modified on Mon Nov  3 20:00:35 2025) since it couldn't be found locally at iwslt2017, or remotely on the Hugging Face Hub.
2025-11-05 22:11:28 - INFO - vocab size: 10000
2025-11-05 22:11:55 - INFO - Epoch 1/5, Batch 500, Loss: 2.3707
2025-11-05 22:12:19 - INFO - Epoch 1/5, Batch 1000, Loss: 2.1882
2025-11-05 22:12:42 - INFO - Epoch 1/5, Batch 1500, Loss: 2.0848
2025-11-05 22:13:05 - INFO - Epoch 1/5, Batch 2000, Loss: 2.0196
2025-11-05 22:13:29 - INFO - Epoch 1/5, Batch 2500, Loss: 1.9640
2025-11-05 22:13:53 - INFO - Epoch 1/5, Batch 3000, Loss: 1.9196
2025-11-05 22:14:15 - INFO - Epoch 1/5, Batch 3500, Loss: 1.8863
2025-11-05 22:14:37 - INFO - Epoch 1/5, Batch 4000, Loss: 1.8524
2025-11-05 22:14:59 - INFO - Epoch 1/5, Batch 4500, Loss: 1.8237
2025-11-05 22:15:21 - INFO - Epoch 1/5, Batch 5000, Loss: 1.7947
2025-11-05 22:15:43 - INFO - Epoch 1/5, Batch 5500, Loss: 1.7719
2025-11-05 22:16:04 - INFO - Epoch 1/5, Batch 6000, Loss: 1.7507
2025-11-05 22:16:26 - INFO - Epoch 1/5, Batch 6500, Loss: 1.7326
2025-11-05 22:16:48 - INFO - Epoch 1/5, Batch 7000, Loss: 1.7151
2025-11-05 22:17:10 - INFO - Epoch 1/5, Batch 7500, Loss: 1.6962
2025-11-05 22:17:31 - INFO - Epoch 1/5, Batch 8000, Loss: 1.6797
2025-11-05 22:17:55 - INFO - Epoch 1/5, Batch 8500, Loss: 1.6657
2025-11-05 22:18:17 - INFO - Epoch 1/5, Batch 9000, Loss: 1.6513
2025-11-05 22:18:39 - INFO - Epoch 1/5, Batch 9500, Loss: 1.6391
2025-11-05 22:19:00 - INFO - Epoch 1/5, Batch 10000, Loss: 1.6268
2025-11-05 22:19:22 - INFO - Epoch 1/5, Batch 10500, Loss: 1.6156
2025-11-05 22:19:46 - INFO - Epoch 1/5, Batch 11000, Loss: 1.6042
2025-11-05 22:20:10 - INFO - Epoch 1/5, Batch 11500, Loss: 1.5941
2025-11-05 22:20:33 - INFO - Epoch 1/5, Batch 12000, Loss: 1.5834
2025-11-05 22:20:56 - INFO - Epoch 1/5, Batch 12500, Loss: 1.5744
2025-11-05 22:21:13 - INFO - Epoch 1/5, Train Loss: 1.5673, Validation Loss: 1.5744, Accuracy: 0.3917, Time: 582.51s
2025-11-05 22:21:14 - INFO - Saved best model to ./results/20251105_221106/best_transformer.pt
2025-11-05 22:21:14 - INFO - Best Validation Loss: 1.2848, Best PPL: 3.6139007067003543
2025-11-05 22:21:35 - INFO - Epoch 2/5, Batch 500, Loss: 1.2988
2025-11-05 22:21:57 - INFO - Epoch 2/5, Batch 1000, Loss: 1.2917
2025-11-05 22:22:19 - INFO - Epoch 2/5, Batch 1500, Loss: 1.2942
2025-11-05 22:22:40 - INFO - Epoch 2/5, Batch 2000, Loss: 1.2924
2025-11-05 22:23:02 - INFO - Epoch 2/5, Batch 2500, Loss: 1.2865
2025-11-05 22:23:24 - INFO - Epoch 2/5, Batch 3000, Loss: 1.2841
2025-11-05 22:23:46 - INFO - Epoch 2/5, Batch 3500, Loss: 1.2835
2025-11-05 22:24:07 - INFO - Epoch 2/5, Batch 4000, Loss: 1.2826
2025-11-05 22:24:29 - INFO - Epoch 2/5, Batch 4500, Loss: 1.2802
2025-11-05 22:24:51 - INFO - Epoch 2/5, Batch 5000, Loss: 1.2774
2025-11-05 22:25:13 - INFO - Epoch 2/5, Batch 5500, Loss: 1.2748
2025-11-05 22:25:34 - INFO - Epoch 2/5, Batch 6000, Loss: 1.2725
2025-11-05 22:25:56 - INFO - Epoch 2/5, Batch 6500, Loss: 1.2699
2025-11-05 22:26:18 - INFO - Epoch 2/5, Batch 7000, Loss: 1.2664
2025-11-05 22:26:40 - INFO - Epoch 2/5, Batch 7500, Loss: 1.2640
2025-11-05 22:27:05 - INFO - Epoch 2/5, Batch 8000, Loss: 1.2617
2025-11-05 22:27:30 - INFO - Epoch 2/5, Batch 8500, Loss: 1.2591
2025-11-05 22:27:55 - INFO - Epoch 2/5, Batch 9000, Loss: 1.2562
2025-11-05 22:28:20 - INFO - Epoch 2/5, Batch 9500, Loss: 1.2547
2025-11-05 22:28:46 - INFO - Epoch 2/5, Batch 10000, Loss: 1.2528
2025-11-05 22:29:12 - INFO - Epoch 2/5, Batch 10500, Loss: 1.2515
2025-11-05 22:29:37 - INFO - Epoch 2/5, Batch 11000, Loss: 1.2488
2025-11-05 22:30:03 - INFO - Epoch 2/5, Batch 11500, Loss: 1.2474
2025-11-05 22:30:28 - INFO - Epoch 2/5, Batch 12000, Loss: 1.2449
2025-11-05 22:30:54 - INFO - Epoch 2/5, Batch 12500, Loss: 1.2429
2025-11-05 22:31:14 - INFO - Epoch 2/5, Train Loss: 1.2411, Validation Loss: 1.2429, Accuracy: 0.4396, Time: 599.55s
2025-11-05 22:31:14 - INFO - Saved best model to ./results/20251105_221106/best_transformer.pt
2025-11-05 22:31:14 - INFO - Best Validation Loss: 1.1374, Best PPL: 3.118755192966303
2025-11-05 22:31:39 - INFO - Epoch 3/5, Batch 500, Loss: 1.1575
2025-11-05 22:32:04 - INFO - Epoch 3/5, Batch 1000, Loss: 1.1628
2025-11-05 22:32:30 - INFO - Epoch 3/5, Batch 1500, Loss: 1.1616
2025-11-05 22:32:55 - INFO - Epoch 3/5, Batch 2000, Loss: 1.1597
2025-11-05 22:33:21 - INFO - Epoch 3/5, Batch 2500, Loss: 1.1599
2025-11-05 22:33:46 - INFO - Epoch 3/5, Batch 3000, Loss: 1.1598
2025-11-05 22:34:12 - INFO - Epoch 3/5, Batch 3500, Loss: 1.1604
2025-11-05 22:34:37 - INFO - Epoch 3/5, Batch 4000, Loss: 1.1624
2025-11-05 22:35:03 - INFO - Epoch 3/5, Batch 4500, Loss: 1.1602
2025-11-05 22:35:28 - INFO - Epoch 3/5, Batch 5000, Loss: 1.1588
2025-11-05 22:35:54 - INFO - Epoch 3/5, Batch 5500, Loss: 1.1585
2025-11-05 22:36:19 - INFO - Epoch 3/5, Batch 6000, Loss: 1.1574
2025-11-05 22:36:41 - INFO - Epoch 3/5, Batch 6500, Loss: 1.1564
2025-11-05 22:37:03 - INFO - Epoch 3/5, Batch 7000, Loss: 1.1558
2025-11-05 22:37:28 - INFO - Epoch 3/5, Batch 7500, Loss: 1.1539
2025-11-05 22:37:53 - INFO - Epoch 3/5, Batch 8000, Loss: 1.1529
2025-11-05 22:38:19 - INFO - Epoch 3/5, Batch 8500, Loss: 1.1515
2025-11-05 22:38:44 - INFO - Epoch 3/5, Batch 9000, Loss: 1.1510
2025-11-05 22:39:09 - INFO - Epoch 3/5, Batch 9500, Loss: 1.1492
2025-11-05 22:39:34 - INFO - Epoch 3/5, Batch 10000, Loss: 1.1487
2025-11-05 22:39:59 - INFO - Epoch 3/5, Batch 10500, Loss: 1.1477
2025-11-05 22:40:25 - INFO - Epoch 3/5, Batch 11000, Loss: 1.1465
2025-11-05 22:40:50 - INFO - Epoch 3/5, Batch 11500, Loss: 1.1453
2025-11-05 22:41:13 - INFO - Epoch 3/5, Batch 12000, Loss: 1.1436
2025-11-05 22:41:35 - INFO - Epoch 3/5, Batch 12500, Loss: 1.1434
2025-11-05 22:41:52 - INFO - Epoch 3/5, Train Loss: 1.1424, Validation Loss: 1.1434, Accuracy: 0.4577, Time: 637.85s
2025-11-05 22:41:52 - INFO - Saved best model to ./results/20251105_221106/best_transformer.pt
2025-11-05 22:41:52 - INFO - Best Validation Loss: 1.0711, Best PPL: 2.918587823935496
2025-11-05 22:42:14 - INFO - Epoch 4/5, Batch 500, Loss: 1.0976
2025-11-05 22:42:36 - INFO - Epoch 4/5, Batch 1000, Loss: 1.0907
2025-11-05 22:42:58 - INFO - Epoch 4/5, Batch 1500, Loss: 1.0876
2025-11-05 22:43:20 - INFO - Epoch 4/5, Batch 2000, Loss: 1.0898
2025-11-05 22:43:42 - INFO - Epoch 4/5, Batch 2500, Loss: 1.0885
2025-11-05 22:44:04 - INFO - Epoch 4/5, Batch 3000, Loss: 1.0872
2025-11-05 22:44:26 - INFO - Epoch 4/5, Batch 3500, Loss: 1.0874
2025-11-05 22:44:48 - INFO - Epoch 4/5, Batch 4000, Loss: 1.0851
2025-11-05 22:45:10 - INFO - Epoch 4/5, Batch 4500, Loss: 1.0850
2025-11-05 22:45:32 - INFO - Epoch 4/5, Batch 5000, Loss: 1.0843
2025-11-05 22:45:54 - INFO - Epoch 4/5, Batch 5500, Loss: 1.0842
2025-11-05 22:46:16 - INFO - Epoch 4/5, Batch 6000, Loss: 1.0834
2025-11-05 22:46:38 - INFO - Epoch 4/5, Batch 6500, Loss: 1.0842
2025-11-05 22:47:00 - INFO - Epoch 4/5, Batch 7000, Loss: 1.0843
2025-11-05 22:47:22 - INFO - Epoch 4/5, Batch 7500, Loss: 1.0836
2025-11-05 22:47:44 - INFO - Epoch 4/5, Batch 8000, Loss: 1.0839
2025-11-05 22:48:06 - INFO - Epoch 4/5, Batch 8500, Loss: 1.0838
2025-11-05 22:48:30 - INFO - Epoch 4/5, Batch 9000, Loss: 1.0833
2025-11-05 22:48:56 - INFO - Epoch 4/5, Batch 9500, Loss: 1.0828
2025-11-05 22:49:19 - INFO - Epoch 4/5, Batch 10000, Loss: 1.0829
2025-11-05 22:49:41 - INFO - Epoch 4/5, Batch 10500, Loss: 1.0822
2025-11-05 22:50:04 - INFO - Epoch 4/5, Batch 11000, Loss: 1.0821
2025-11-05 22:50:26 - INFO - Epoch 4/5, Batch 11500, Loss: 1.0828
2025-11-05 22:50:48 - INFO - Epoch 4/5, Batch 12000, Loss: 1.0827
2025-11-05 22:51:10 - INFO - Epoch 4/5, Batch 12500, Loss: 1.0825
2025-11-05 22:51:27 - INFO - Epoch 4/5, Train Loss: 1.0819, Validation Loss: 1.0825, Accuracy: 0.4831, Time: 574.50s
2025-11-05 22:51:27 - INFO - Saved best model to ./results/20251105_221106/best_transformer.pt
2025-11-05 22:51:27 - INFO - Best Validation Loss: 1.0289, Best PPL: 2.7979143470514987
2025-11-05 22:51:49 - INFO - Epoch 5/5, Batch 500, Loss: 1.0254
2025-11-05 22:52:11 - INFO - Epoch 5/5, Batch 1000, Loss: 1.0268
2025-11-05 22:52:33 - INFO - Epoch 5/5, Batch 1500, Loss: 1.0302
2025-11-05 22:52:54 - INFO - Epoch 5/5, Batch 2000, Loss: 1.0311
2025-11-05 22:53:16 - INFO - Epoch 5/5, Batch 2500, Loss: 1.0362
2025-11-05 22:53:38 - INFO - Epoch 5/5, Batch 3000, Loss: 1.0383
2025-11-05 22:54:00 - INFO - Epoch 5/5, Batch 3500, Loss: 1.0409
2025-11-05 22:54:22 - INFO - Epoch 5/5, Batch 4000, Loss: 1.0420
2025-11-05 22:54:44 - INFO - Epoch 5/5, Batch 4500, Loss: 1.0441
2025-11-05 22:55:06 - INFO - Epoch 5/5, Batch 5000, Loss: 1.0428
2025-11-05 22:55:28 - INFO - Epoch 5/5, Batch 5500, Loss: 1.0439
2025-11-05 22:55:50 - INFO - Epoch 5/5, Batch 6000, Loss: 1.0448
2025-11-05 22:56:12 - INFO - Epoch 5/5, Batch 6500, Loss: 1.0427
2025-11-05 22:56:34 - INFO - Epoch 5/5, Batch 7000, Loss: 1.0423
2025-11-05 22:56:56 - INFO - Epoch 5/5, Batch 7500, Loss: 1.0426
2025-11-05 22:57:18 - INFO - Epoch 5/5, Batch 8000, Loss: 1.0429
2025-11-05 22:57:40 - INFO - Epoch 5/5, Batch 8500, Loss: 1.0435
2025-11-05 22:58:02 - INFO - Epoch 5/5, Batch 9000, Loss: 1.0439
2025-11-05 22:58:24 - INFO - Epoch 5/5, Batch 9500, Loss: 1.0440
2025-11-05 22:58:46 - INFO - Epoch 5/5, Batch 10000, Loss: 1.0438
2025-11-05 22:59:07 - INFO - Epoch 5/5, Batch 10500, Loss: 1.0437
2025-11-05 22:59:29 - INFO - Epoch 5/5, Batch 11000, Loss: 1.0445
2025-11-05 22:59:51 - INFO - Epoch 5/5, Batch 11500, Loss: 1.0449
2025-11-05 23:00:13 - INFO - Epoch 5/5, Batch 12000, Loss: 1.0454
2025-11-05 23:00:35 - INFO - Epoch 5/5, Batch 12500, Loss: 1.0448
2025-11-05 23:00:53 - INFO - Epoch 5/5, Train Loss: 1.0446, Validation Loss: 1.0448, Accuracy: 0.4843, Time: 564.71s
2025-11-05 23:00:53 - INFO - Saved best model to ./results/20251105_221106/best_transformer.pt
2025-11-05 23:00:53 - INFO - Best Validation Loss: 1.0046, Best PPL: 2.7309073625886997
2025-11-05 23:00:54 - INFO - Saved training parameters to ./results/20251105_221106/config.json
