2025-11-05 18:34:01 - INFO - Training Configuration:
2025-11-05 18:34:01 - INFO - d_model: 128
2025-11-05 18:34:01 - INFO - n_head: 4
2025-11-05 18:34:01 - INFO - n_enc_layers: 2
2025-11-05 18:34:01 - INFO - n_dec_layers: 2
2025-11-05 18:34:01 - INFO - d_ff: 512
2025-11-05 18:34:01 - INFO - dropout: 0.1
2025-11-05 18:34:01 - INFO - batch_size: 16
2025-11-05 18:34:01 - INFO - epochs: 5
2025-11-05 18:34:01 - INFO - lr: 0.001
2025-11-05 18:34:01 - INFO - seed: 42
2025-11-05 18:34:06 - WARNING - Using the latest cached version of the module from /home/ic611/.cache/huggingface/modules/datasets_modules/datasets/iwslt2017/03ce9110373117c6f6687719f49f269486a8cd49dcad2527993a316cd4b6ad49 (last modified on Mon Nov  3 20:00:35 2025) since it couldn't be found locally at iwslt2017, or remotely on the Hugging Face Hub.
2025-11-05 18:34:23 - INFO - vocab size: 10000
2025-11-05 18:34:49 - INFO - Epoch 1/5, Batch 500, Loss: 2.3694
2025-11-05 18:35:11 - INFO - Epoch 1/5, Batch 1000, Loss: 2.1867
2025-11-05 18:35:33 - INFO - Epoch 1/5, Batch 1500, Loss: 2.0835
2025-11-05 18:35:54 - INFO - Epoch 1/5, Batch 2000, Loss: 2.0180
2025-11-05 18:36:16 - INFO - Epoch 1/5, Batch 2500, Loss: 1.9617
2025-11-05 18:36:38 - INFO - Epoch 1/5, Batch 3000, Loss: 1.9165
2025-11-05 18:36:59 - INFO - Epoch 1/5, Batch 3500, Loss: 1.8831
2025-11-05 18:37:21 - INFO - Epoch 1/5, Batch 4000, Loss: 1.8489
2025-11-05 18:37:43 - INFO - Epoch 1/5, Batch 4500, Loss: 1.8199
2025-11-05 18:38:05 - INFO - Epoch 1/5, Batch 5000, Loss: 1.7908
2025-11-05 18:38:26 - INFO - Epoch 1/5, Batch 5500, Loss: 1.7679
2025-11-05 18:38:48 - INFO - Epoch 1/5, Batch 6000, Loss: 1.7463
2025-11-05 18:39:10 - INFO - Epoch 1/5, Batch 6500, Loss: 1.7280
2025-11-05 18:39:31 - INFO - Epoch 1/5, Batch 7000, Loss: 1.7104
2025-11-05 18:39:54 - INFO - Epoch 1/5, Batch 7500, Loss: 1.6915
2025-11-05 18:40:17 - INFO - Epoch 1/5, Batch 8000, Loss: 1.6748
2025-11-05 18:40:41 - INFO - Epoch 1/5, Batch 8500, Loss: 1.6606
2025-11-05 18:41:04 - INFO - Epoch 1/5, Batch 9000, Loss: 1.6461
2025-11-05 18:41:29 - INFO - Epoch 1/5, Batch 9500, Loss: 1.6336
2025-11-05 18:41:54 - INFO - Epoch 1/5, Batch 10000, Loss: 1.6212
2025-11-05 18:42:19 - INFO - Epoch 1/5, Batch 10500, Loss: 1.6097
2025-11-05 18:42:42 - INFO - Epoch 1/5, Batch 11000, Loss: 1.5981
2025-11-05 18:43:04 - INFO - Epoch 1/5, Batch 11500, Loss: 1.5879
2025-11-05 18:43:27 - INFO - Epoch 1/5, Batch 12000, Loss: 1.5770
2025-11-05 18:43:50 - INFO - Epoch 1/5, Batch 12500, Loss: 1.5678
2025-11-05 18:44:07 - INFO - Epoch 1/5, Train Loss: 1.5606, Validation Loss: 1.5678, Accuracy: 0.3951, Time: 581.08s
2025-11-05 18:44:07 - INFO - Saved best model to ./results/20251105_183401/best_transformer.pt
2025-11-05 18:44:07 - INFO - Best Validation Loss: 1.2799, Best PPL: 3.5963121360631516
2025-11-05 18:44:28 - INFO - Epoch 2/5, Batch 500, Loss: 1.2874
2025-11-05 18:44:50 - INFO - Epoch 2/5, Batch 1000, Loss: 1.2803
2025-11-05 18:45:13 - INFO - Epoch 2/5, Batch 1500, Loss: 1.2836
2025-11-05 18:45:36 - INFO - Epoch 2/5, Batch 2000, Loss: 1.2815
2025-11-05 18:46:00 - INFO - Epoch 2/5, Batch 2500, Loss: 1.2759
2025-11-05 18:46:21 - INFO - Epoch 2/5, Batch 3000, Loss: 1.2738
2025-11-05 18:46:43 - INFO - Epoch 2/5, Batch 3500, Loss: 1.2734
2025-11-05 18:47:04 - INFO - Epoch 2/5, Batch 4000, Loss: 1.2725
2025-11-05 18:47:26 - INFO - Epoch 2/5, Batch 4500, Loss: 1.2701
2025-11-05 18:47:47 - INFO - Epoch 2/5, Batch 5000, Loss: 1.2674
2025-11-05 18:48:09 - INFO - Epoch 2/5, Batch 5500, Loss: 1.2651
2025-11-05 18:48:30 - INFO - Epoch 2/5, Batch 6000, Loss: 1.2626
2025-11-05 18:48:53 - INFO - Epoch 2/5, Batch 6500, Loss: 1.2600
2025-11-05 18:49:15 - INFO - Epoch 2/5, Batch 7000, Loss: 1.2565
2025-11-05 18:49:37 - INFO - Epoch 2/5, Batch 7500, Loss: 1.2540
2025-11-05 18:49:59 - INFO - Epoch 2/5, Batch 8000, Loss: 1.2516
2025-11-05 18:50:20 - INFO - Epoch 2/5, Batch 8500, Loss: 1.2491
2025-11-05 18:50:42 - INFO - Epoch 2/5, Batch 9000, Loss: 1.2461
2025-11-05 18:51:04 - INFO - Epoch 2/5, Batch 9500, Loss: 1.2445
2025-11-05 18:51:25 - INFO - Epoch 2/5, Batch 10000, Loss: 1.2427
2025-11-05 18:51:47 - INFO - Epoch 2/5, Batch 10500, Loss: 1.2412
2025-11-05 18:52:09 - INFO - Epoch 2/5, Batch 11000, Loss: 1.2385
2025-11-05 18:52:31 - INFO - Epoch 2/5, Batch 11500, Loss: 1.2370
2025-11-05 18:52:52 - INFO - Epoch 2/5, Batch 12000, Loss: 1.2345
2025-11-05 18:53:14 - INFO - Epoch 2/5, Batch 12500, Loss: 1.2324
2025-11-05 18:53:31 - INFO - Epoch 2/5, Train Loss: 1.2307, Validation Loss: 1.2324, Accuracy: 0.4441, Time: 563.59s
2025-11-05 18:53:31 - INFO - Saved best model to ./results/20251105_183401/best_transformer.pt
2025-11-05 18:53:31 - INFO - Best Validation Loss: 1.1281, Best PPL: 3.089919492603704
2025-11-05 18:53:53 - INFO - Epoch 3/5, Batch 500, Loss: 1.1466
2025-11-05 18:54:15 - INFO - Epoch 3/5, Batch 1000, Loss: 1.1512
2025-11-05 18:54:36 - INFO - Epoch 3/5, Batch 1500, Loss: 1.1491
2025-11-05 18:54:58 - INFO - Epoch 3/5, Batch 2000, Loss: 1.1473
2025-11-05 18:55:20 - INFO - Epoch 3/5, Batch 2500, Loss: 1.1471
2025-11-05 18:55:41 - INFO - Epoch 3/5, Batch 3000, Loss: 1.1471
2025-11-05 18:56:03 - INFO - Epoch 3/5, Batch 3500, Loss: 1.1475
2025-11-05 18:56:25 - INFO - Epoch 3/5, Batch 4000, Loss: 1.1494
2025-11-05 18:56:47 - INFO - Epoch 3/5, Batch 4500, Loss: 1.1470
2025-11-05 18:57:08 - INFO - Epoch 3/5, Batch 5000, Loss: 1.1458
2025-11-05 18:57:30 - INFO - Epoch 3/5, Batch 5500, Loss: 1.1455
2025-11-05 18:57:52 - INFO - Epoch 3/5, Batch 6000, Loss: 1.1444
2025-11-05 18:58:13 - INFO - Epoch 3/5, Batch 6500, Loss: 1.1434
2025-11-05 18:58:35 - INFO - Epoch 3/5, Batch 7000, Loss: 1.1428
2025-11-05 18:58:57 - INFO - Epoch 3/5, Batch 7500, Loss: 1.1409
2025-11-05 18:59:18 - INFO - Epoch 3/5, Batch 8000, Loss: 1.1399
2025-11-05 18:59:40 - INFO - Epoch 3/5, Batch 8500, Loss: 1.1386
2025-11-05 19:00:02 - INFO - Epoch 3/5, Batch 9000, Loss: 1.1379
2025-11-05 19:00:24 - INFO - Epoch 3/5, Batch 9500, Loss: 1.1362
2025-11-05 19:00:46 - INFO - Epoch 3/5, Batch 10000, Loss: 1.1356
2025-11-05 19:01:07 - INFO - Epoch 3/5, Batch 10500, Loss: 1.1345
2025-11-05 19:01:29 - INFO - Epoch 3/5, Batch 11000, Loss: 1.1332
2025-11-05 19:01:51 - INFO - Epoch 3/5, Batch 11500, Loss: 1.1319
2025-11-05 19:02:13 - INFO - Epoch 3/5, Batch 12000, Loss: 1.1301
2025-11-05 19:02:35 - INFO - Epoch 3/5, Batch 12500, Loss: 1.1298
2025-11-05 19:02:52 - INFO - Epoch 3/5, Train Loss: 1.1288, Validation Loss: 1.1298, Accuracy: 0.4606, Time: 560.28s
2025-11-05 19:02:52 - INFO - Saved best model to ./results/20251105_183401/best_transformer.pt
2025-11-05 19:02:52 - INFO - Best Validation Loss: 1.0608, Best PPL: 2.888643746129639
2025-11-05 19:03:14 - INFO - Epoch 4/5, Batch 500, Loss: 1.0803
2025-11-05 19:03:36 - INFO - Epoch 4/5, Batch 1000, Loss: 1.0738
2025-11-05 19:03:58 - INFO - Epoch 4/5, Batch 1500, Loss: 1.0714
2025-11-05 19:04:20 - INFO - Epoch 4/5, Batch 2000, Loss: 1.0740
2025-11-05 19:04:41 - INFO - Epoch 4/5, Batch 2500, Loss: 1.0728
2025-11-05 19:05:03 - INFO - Epoch 4/5, Batch 3000, Loss: 1.0715
2025-11-05 19:05:25 - INFO - Epoch 4/5, Batch 3500, Loss: 1.0718
2025-11-05 19:05:47 - INFO - Epoch 4/5, Batch 4000, Loss: 1.0694
2025-11-05 19:06:09 - INFO - Epoch 4/5, Batch 4500, Loss: 1.0694
2025-11-05 19:06:31 - INFO - Epoch 4/5, Batch 5000, Loss: 1.0688
2025-11-05 19:06:52 - INFO - Epoch 4/5, Batch 5500, Loss: 1.0689
2025-11-05 19:07:14 - INFO - Epoch 4/5, Batch 6000, Loss: 1.0682
2025-11-05 19:07:36 - INFO - Epoch 4/5, Batch 6500, Loss: 1.0690
2025-11-05 19:07:58 - INFO - Epoch 4/5, Batch 7000, Loss: 1.0690
2025-11-05 19:08:20 - INFO - Epoch 4/5, Batch 7500, Loss: 1.0682
2025-11-05 19:08:42 - INFO - Epoch 4/5, Batch 8000, Loss: 1.0685
2025-11-05 19:09:04 - INFO - Epoch 4/5, Batch 8500, Loss: 1.0683
2025-11-05 19:09:25 - INFO - Epoch 4/5, Batch 9000, Loss: 1.0678
2025-11-05 19:09:47 - INFO - Epoch 4/5, Batch 9500, Loss: 1.0671
2025-11-05 19:10:09 - INFO - Epoch 4/5, Batch 10000, Loss: 1.0673
2025-11-05 19:10:30 - INFO - Epoch 4/5, Batch 10500, Loss: 1.0665
2025-11-05 19:10:52 - INFO - Epoch 4/5, Batch 11000, Loss: 1.0663
2025-11-05 19:11:14 - INFO - Epoch 4/5, Batch 11500, Loss: 1.0671
2025-11-05 19:11:36 - INFO - Epoch 4/5, Batch 12000, Loss: 1.0669
2025-11-05 19:11:58 - INFO - Epoch 4/5, Batch 12500, Loss: 1.0667
2025-11-05 19:12:15 - INFO - Epoch 4/5, Train Loss: 1.0660, Validation Loss: 1.0667, Accuracy: 0.4887, Time: 561.90s
2025-11-05 19:12:15 - INFO - Saved best model to ./results/20251105_183401/best_transformer.pt
2025-11-05 19:12:15 - INFO - Best Validation Loss: 1.0147, Best PPL: 2.7585711428494615
2025-11-05 19:12:36 - INFO - Epoch 5/5, Batch 500, Loss: 1.0097
2025-11-05 19:12:58 - INFO - Epoch 5/5, Batch 1000, Loss: 1.0108
2025-11-05 19:13:20 - INFO - Epoch 5/5, Batch 1500, Loss: 1.0138
2025-11-05 19:13:42 - INFO - Epoch 5/5, Batch 2000, Loss: 1.0144
2025-11-05 19:14:03 - INFO - Epoch 5/5, Batch 2500, Loss: 1.0191
2025-11-05 19:14:25 - INFO - Epoch 5/5, Batch 3000, Loss: 1.0213
2025-11-05 19:14:47 - INFO - Epoch 5/5, Batch 3500, Loss: 1.0240
2025-11-05 19:15:08 - INFO - Epoch 5/5, Batch 4000, Loss: 1.0253
2025-11-05 19:15:30 - INFO - Epoch 5/5, Batch 4500, Loss: 1.0272
2025-11-05 19:15:52 - INFO - Epoch 5/5, Batch 5000, Loss: 1.0259
2025-11-05 19:16:13 - INFO - Epoch 5/5, Batch 5500, Loss: 1.0268
2025-11-05 19:16:35 - INFO - Epoch 5/5, Batch 6000, Loss: 1.0279
2025-11-05 19:16:57 - INFO - Epoch 5/5, Batch 6500, Loss: 1.0259
2025-11-05 19:17:18 - INFO - Epoch 5/5, Batch 7000, Loss: 1.0255
2025-11-05 19:17:40 - INFO - Epoch 5/5, Batch 7500, Loss: 1.0257
2025-11-05 19:18:02 - INFO - Epoch 5/5, Batch 8000, Loss: 1.0260
2025-11-05 19:18:24 - INFO - Epoch 5/5, Batch 8500, Loss: 1.0266
2025-11-05 19:18:46 - INFO - Epoch 5/5, Batch 9000, Loss: 1.0270
2025-11-05 19:19:07 - INFO - Epoch 5/5, Batch 9500, Loss: 1.0270
2025-11-05 19:19:29 - INFO - Epoch 5/5, Batch 10000, Loss: 1.0268
2025-11-05 19:19:51 - INFO - Epoch 5/5, Batch 10500, Loss: 1.0266
2025-11-05 19:20:12 - INFO - Epoch 5/5, Batch 11000, Loss: 1.0274
2025-11-05 19:20:36 - INFO - Epoch 5/5, Batch 11500, Loss: 1.0279
2025-11-05 19:21:02 - INFO - Epoch 5/5, Batch 12000, Loss: 1.0283
2025-11-05 19:21:27 - INFO - Epoch 5/5, Batch 12500, Loss: 1.0278
2025-11-05 19:21:45 - INFO - Epoch 5/5, Train Loss: 1.0276, Validation Loss: 1.0278, Accuracy: 0.4914, Time: 569.48s
2025-11-05 19:21:45 - INFO - Saved best model to ./results/20251105_183401/best_transformer.pt
2025-11-05 19:21:45 - INFO - Best Validation Loss: 0.9877, Best PPL: 2.6851594978722453
2025-11-05 19:21:46 - INFO - Saved training parameters to ./results/20251105_183401/config.json
